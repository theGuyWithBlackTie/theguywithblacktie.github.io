---
layout: post
title: 'The last blog you need to understand DSPy'
date: 2025-06-28 15:00 +0530
math: true
description: 'Exploring the framework for programming the large language models, not prompting'
toc: true
image:
    path: assets/headers/dspy-header.png
    alt: DSPy
hidden: false
---

With the rise of LLMs, lots of data scientists are spending ton of their time on prompt engineering to test different LLMs performances on their projects. Every week a new foundational model is released which is better and cheaper than all the previous releases, tempting all of the data scientists to test one more LLM for their projects. A satisfactory LLM testing requires ample time spent on prompt engineering to cover all the edge cases successfully, and to understand all the nuances a new LLM has. This takes weeks to be done.

In the age of AI (or automation), how about a tool that can automate the prompt engineering for data scientist while saving their time to focus on actual data science work i.e. isights generation from statistics, developing AI model architectures. That's where DSPy comes in; it offers prompt tuning for different LLMs, tuning LLM weights and AI agents.

# Introduction
[DSPy](https://github.com/stanfordnlp/dspy), pronounced as <i>dee-s-pie</i>, is developed by researcher Omar Khattab and others at <b>Stanford NLP</b> group. The [DSPy research paper](https://arxiv.org/abs/2310.03714) argues that existing LLM pipelines are typically implemented using hard-coded "prompt-temmplates" that are discovered via trial and error, whereas DSPy provides more systematic approach for developing and optimizing LLM pipelines.

## Configuring LLMs
The very first step in DSPy in initializing a language model of your choice for the future operations in the code. You can do this as follows:

```
import dspy

global_llm = dspy.LM('openai/gpt-4o-mini', api_key = <API_KEY>, temperature = 0.7, max_tokens = 3000, stop = None, cache = False)
dspy.configure(global_llm)
```

`dspy.LM` initializes the LLM of your choice and `dspy.configure` makes the initialized LLM as default LLM for the whole program. DSPy supports all the LLM providers, it can be found [here](https://dspy.ai/learn/programming/language_models/#__tabbed_1_1).

`temperature`, `cache`, `max_tokens` are some of the attributes one can configure while initializing the LLM. More attributes can be read [here](https://dspy.ai/api/models/LM/).

> By default LLMs in DSPy are cached. If you repeat the same call, you will get the same outputs. Caching can be turned off by setting `cache=False`. When using small LMs like gpt-4o-mini, you may receive the same output even with cache=False. In Jupyter Notebook, check if caching is active by observing the response timeâ€”instant responses indicate caching, while delays suggest actual LLM calls.
{: .prompt-info}

LMs configured above can be called directly as below:
```
global_llm("Classify this sentiment into either Positive, Neutral or Negative: I loved the product. The service is worst though.", temperature = 0.2)
global_llm(messages=[{
    "role": "user",
    "content": "Classify this sentiment into either Positive, Neutral or Negative: I loved the product. The service is worst though."
}], temperature = 0.5)
```

## Signature
Prompts comprises of three parts <i>viz.</i>(a) what would be the inputs & outputs (b) describing the task and (c) prompting techniques such as <i>Chain of Thought</i>, <i>ReAct</i>, <i>Program of Though</i>, etc. <i>Signature</i> in DSPy is a programmatic way of defining the first two parts of prompt for your task.

Signature has two attributes; `InputField()` to define inputs and `OutputField()` to define outputs to the prompt. The task description is declared as the <i>docstrings</i> in the `Signature` class. Let's say we have task of sentiment classification of user reviews, the `Signature` is defined as:
```
import dspy

class SentimentSignature(dspy.Signature):
    """
    You would be given an input text; and you need to classify it into strictly these three sentiments: (a)Positive, (b)Neutral or (c)Negative.
    """
    sentiment_text = dspy.InputField()
    sentiment_classification = dspy.OutputField()
```

With the above defined signature and using standard prompting, the <i>system prompt</i> would be this:
```
Your input fields are:
1. `sentiment_text` (str):
Your output fields are:
1. `sentiment_classification` (str):
All interactions will be structured in the following way, with the appropriate values filled in.

[[ ## sentiment_text ## ]]
{sentiment_text}

[[ ## sentiment_classification ## ]]
{sentiment_classification}

[[ ## completed ## ]]
In adhering to this structure, your objective is: 
        You would be given an input text; and you need to classify it into strictly these three sentiments: (a)Positive, (b)Neutral or (c)Negative.
```

The user prompt will be be this:
```
[[ ## sentiment_text ## ]]
<The input text whose sentiment need to be identified would be here>

Respond with the corresponding output fields, starting with the field `[[ ## sentiment_classification ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.
```

In the <i>system prompt</i>, `sentiment_text` and `sentiment_classification` present as these are the variables (with the same text) declared in the `SentimentSignature` signature. The <i>docstring</i> in the `SentimentSignature` class is present at the end of the <i>system prompt</i>.
